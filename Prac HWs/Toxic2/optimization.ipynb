{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDClassifier:\n",
    "    \"\"\"\n",
    "    Реализация метода градиентного спуска для произвольного\n",
    "    оракула, соответствующего спецификации оракулов из модуля oracles.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_function, step_alpha=1, step_beta=0, \n",
    "                 tolerance=1e-5, max_iter=1000, **kwargs):\n",
    "        \"\"\"\n",
    "        loss_function - строка, отвечающая за функцию потерь классификатора. \n",
    "        Может принимать значения:\n",
    "        - 'binary_logistic' - бинарная логистическая регрессия\n",
    "\n",
    "        step_alpha - float, параметр выбора шага из текста задания\n",
    "\n",
    "        step_beta- float, параметр выбора шага из текста задания\n",
    "\n",
    "        tolerance - точность, по достижении которой, необходимо прекратить оптимизацию.\n",
    "        Необходимо использовать критерий выхода по модулю разности соседних значений функции:\n",
    "        если |f(x_{k+1}) - f(x_{k})| < tolerance: то выход \n",
    "\n",
    "        max_iter - максимальное число итераций     \n",
    "\n",
    "        **kwargs - аргументы, необходимые для инициализации\n",
    "        \"\"\"\n",
    "        self.loss_function = loss_function\n",
    "        self.step_alpha = step_alpha\n",
    "        self.step_beta = step_beta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "#         self.trashhold = 0.5\n",
    "        if 'l2_coef' in kwargs.keys():\n",
    "            self.l2_coef = kwargs.pop('l2_coef')\n",
    "        else:\n",
    "            self.l2_coef = 0\n",
    "#         if 'iter_acc' in kwargs.keys():\n",
    "#             self.iter_acc = kwargs.pop('iter_acc')\n",
    "#         else:\n",
    "#             self.iter_acc = 0\n",
    "        if 'w_0' in kwargs.keys():\n",
    "            self.w_0 = w_0\n",
    "            \n",
    "    @staticmethod\n",
    "    def softmax(X, w):\n",
    "        alp = X.dot(w)\n",
    "        #alp -= np.max(alp)\n",
    "        #exp_l = np.exp(alp)\n",
    "        exp_l = np.exp(alp - np.max(alp, axis=1)[:, np.newaxis])\n",
    "        return exp_l / np.sum(exp_l, axis=1, keepdims=True)\n",
    "   \n",
    "    @staticmethod\n",
    "    def loss_bc(X, y, w, l2_coef):\n",
    "        loss = np.logaddexp(0, (-y*X.dot(w))).mean() + (l2_coef/2)*(w**2).sum()\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X, y, w_0=None, trace=False):\n",
    "        \"\"\"\n",
    "        Обучение метода по выборке X с ответами y\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        y - одномерный numpy array\n",
    "        \n",
    "        w_0 - начальное приближение в методе\n",
    "        \n",
    "        trace - переменная типа bool\n",
    "      \n",
    "        Если trace = True, то метод должен вернуть словарь history, содержащий информацию \n",
    "        о поведении метода. Длина словаря history = количество итераций + 1 (начальное приближение)\n",
    "        \n",
    "        history['time']: list of floats, содержит интервалы времени между двумя итерациями метода\n",
    "        history['func']: list of floats, содержит значения функции на каждой итерации\n",
    "        (0 для самой первой точки)\n",
    "        \"\"\"\n",
    "        if w_0 is None:\n",
    "            w_0 = np.zeros(X.shape[1])\n",
    "            \n",
    "        self.w = w_0.copy()\n",
    "        if trace == True:\n",
    "            history = {'time': [0], 'func': [self.loss_bc(X, y, self.w, self.l2_coef)]}\n",
    "#             if self.iter_acc is True:\n",
    "#                 acc = [self.accuracy(x_test, y_test)]\n",
    "            start_time = time.time()\n",
    "        for k in range(1, self.max_iter+1):\n",
    "            if k>2 and self.tolerance != 0:\n",
    "                if np.abs(history['func'][-2] - history['func'][-1]) < self.tolerance:\n",
    "                    break\n",
    "            nu = self.step_alpha / (k**self.step_beta)\n",
    "            gradient = self.get_gradient(X, y)\n",
    "            self.w -= nu *gradient\n",
    "            w_l = self.w\n",
    "            if trace == True:\n",
    "                history['time'].append(time.time() - start_time)\n",
    "                loss = self.loss_bc(X, y, self.w, self.l2_coef)\n",
    "                history['func'].append(loss)\n",
    "#                 if self.iter_acc is True:\n",
    "#                     acc.append(self.accuracy(x_test, y_test))\n",
    "                start_time = time.time()\n",
    "        if trace == True:\n",
    "#             if self.iter_acc == True:\n",
    "#                 return history, acc\n",
    "#             else:\n",
    "                return history\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Получение меток ответов на выборке X\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        return: одномерный numpy array с предсказаниями\n",
    "        \"\"\"\n",
    "        return np.where((expit(X.dot(self.w)) >= self.trashhold) == True, 1, -1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Получение вероятностей принадлежности X к классу k\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        return: двумерной numpy array, [i, k] значение соответветствует вероятности\n",
    "        принадлежности i-го объекта к классу k \n",
    "        \"\"\"\n",
    "        return expit(X.dot(self.w))\n",
    "        \n",
    "    def get_objective(self, X, y):\n",
    "        \"\"\"\n",
    "        Получение значения целевой функции на выборке X с ответами y\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "        \n",
    "        return: float\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def get_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        Получение значения градиента функции на выборке X с ответами y\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "        \n",
    "        return: numpy array, размерность зависит от задачи\n",
    "        \"\"\"\n",
    "        grad = X.T.dot((expit(y*X.dot(self.w))-1)*y)/X.shape[0] + self.l2_coef * self.w\n",
    "        return grad\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Получение значения весов функционала\n",
    "        \"\"\"    \n",
    "        return self.w\n",
    "    \n",
    "#     def accuracy(self, X, y_true):\n",
    "#         y_pred = self.predict(X)\n",
    "#         return (y_pred == y_test).sum() / len(y_test)\n",
    "\n",
    "\n",
    "class SGDClassifier(GDClassifier):\n",
    "    \"\"\"\n",
    "    Реализация метода стохастического градиентного спуска для произвольного\n",
    "    оракула, соответствующего спецификации оракулов из модуля oracles.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, loss_function, batch_size, step_alpha=1, step_beta=0, \n",
    "                 tolerance=1e-5, max_iter=1000, random_seed=153, **kwargs):\n",
    "        \"\"\"\n",
    "        loss_function - строка, отвечающая за функцию потерь классификатора. \n",
    "        Может принимать значения:\n",
    "        - 'binary_logistic' - бинарная логистическая регрессия\n",
    "        \n",
    "        batch_size - размер подвыборки, по которой считается градиент\n",
    "        \n",
    "        step_alpha - float, параметр выбора шага из текста задания\n",
    "        \n",
    "        step_beta- float, параметр выбора шага из текста задания\n",
    "        \n",
    "        tolerance - точность, по достижении которой, необходимо прекратить оптимизацию\n",
    "        Необходимо использовать критерий выхода по модулю разности соседних значений функции:\n",
    "        если |f(x_{k+1}) - f(x_{k})| < tolerance: то выход \n",
    "        \n",
    "        \n",
    "        max_iter - максимальное число итераций (эпох)\n",
    "        \n",
    "        random_seed - в начале метода fit необходимо вызвать np.random.seed(random_seed).\n",
    "        Этот параметр нужен для воспроизводимости результатов на разных машинах.\n",
    "        \n",
    "        **kwargs - аргументы, необходимые для инициализации\n",
    "        \"\"\"\n",
    "        self.loss_function = loss_function\n",
    "        self.step_alpha = step_alpha\n",
    "        self.step_beta = step_beta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.rand_seed = random_seed\n",
    "        self.batch_size = batch_size\n",
    "        self.l2_coef = kwargs.pop('l2_coef')\n",
    "        self.trashhold = 0.5\n",
    "        if 'l2_coef' in kwargs.keys():\n",
    "            self.l2_coef = kwargs.pop('l2_coef')\n",
    "        else:\n",
    "            self.l2_coef = 0\n",
    "        if 'iter_acc' in kwargs.keys():\n",
    "            self.iter_acc = kwargs.pop('iter_acc')\n",
    "        else:\n",
    "            self.iter_acc = 0\n",
    "        if 'w_0' in kwargs.keys():\n",
    "            self.w_0 = w_0\n",
    "        \n",
    "    def fit(self, X, y, w_0=None, trace=False, log_freq=1):\n",
    "        \"\"\"\n",
    "        Обучение метода по выборке X с ответами y\n",
    "        \n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        \n",
    "        y - одномерный numpy array\n",
    "                \n",
    "        w_0 - начальное приближение в методе\n",
    "        \n",
    "        Если trace = True, то метод должен вернуть словарь history, содержащий информацию \n",
    "        о поведении метода. Если обновлять history после каждой итерации, метод перестанет \n",
    "        превосходить в скорости метод GD. Поэтому, необходимо обновлять историю метода лишь\n",
    "        после некоторого числа обработанных объектов в зависимости от приближённого номера эпохи.\n",
    "        Приближённый номер эпохи:\n",
    "            {количество объектов, обработанных методом SGD} / {количество объектов в выборке}\n",
    "        \n",
    "        log_freq - float от 0 до 1, параметр, отвечающий за частоту обновления. \n",
    "        Обновление должно проиходить каждый раз, когда разница между двумя значениями приближённого номера эпохи\n",
    "        будет превосходить log_freq.\n",
    "        \n",
    "        history['epoch_num']: list of floats, в каждом элементе списка будет записан приближённый номер эпохи:\n",
    "        history['time']: list of floats, содержит интервалы времени между двумя соседними замерами\n",
    "        history['func']: list of floats, содержит значения функции после текущего приближённого номера эпохи\n",
    "        history['weights_diff']: list of floats, содержит квадрат нормы разности векторов весов с соседних замеров\n",
    "        (0 для самой первой точки)\n",
    "        \"\"\"\n",
    "        np.random.seed(self.rand_seed)\n",
    "        if w_0 is None:\n",
    "            w_0 = np.zeros(X.shape[1])\n",
    "        self.w = w_0.copy()\n",
    "        w_l = self.w.copy()\n",
    "        used_obj_c = 0\n",
    "        epoch_num = 0\n",
    "        if trace == True:\n",
    "            history = {'time': [0], 'func': [self.loss_bc(X, y, self.w, self.l2_coef)],\n",
    "                       'weights_diff': [0], 'epoch_num' : [0]}\n",
    "            if self.iter_acc is True:\n",
    "                acc = [self.accuracy(x_test, y_test)]\n",
    "        start_time = time.time()\n",
    "        for k in range(1 , self.max_iter):\n",
    "            ind = np.random.randint(X.shape[0], size = self.batch_size)\n",
    "            used_obj_c += self.batch_size\n",
    "            epoch_num = used_obj_c/X.shape[0]\n",
    "            if (trace == True) and (used_obj_c/X.shape[0] > log_freq):\n",
    "                history['epoch_num'].append(epoch_num*len(history['epoch_num']))\n",
    "                history['time'].append(time.time() - start_time)\n",
    "                history['weights_diff'].append(((w_l - self.w)**2).sum())\n",
    "                loss = self.loss_bc(X, y, self.w, self.l2_coef)\n",
    "                history['func'].append(loss)\n",
    "                if np.abs(history['func'][-2] - history['func'][-1]) < self.tolerance:\n",
    "                    break\n",
    "                if self.iter_acc is True:\n",
    "                    acc.append(self.accuracy(x_test, y_test))\n",
    "                w_l = self.w\n",
    "                used_obj_c = 0\n",
    "                start_time = time.time()  \n",
    "            nu = self.step_alpha / (k**self.step_beta) \n",
    "            gradient = self.get_gradient(X[ind], y[ind])\n",
    "            self.w -= nu * gradient\n",
    "        if trace == True:\n",
    "            if self.iter_acc == True:\n",
    "                return history, acc\n",
    "            else:\n",
    "                return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
