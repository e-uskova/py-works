{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "from scipy import sparse\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import oracles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDClassifier:\n",
    "    \"\"\"\n",
    "    Реализация метода градиентного спуска для произвольного\n",
    "    оракула, соответствующего спецификации оракулов из модуля oracles.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_function, step_alpha=1, step_beta=0, \n",
    "                 tolerance=1e-5, max_iter=1000, **kwargs):\n",
    "        \"\"\"\n",
    "        loss_function - строка, отвечающая за функцию потерь классификатора. \n",
    "        Может принимать значения:\n",
    "        - 'binary_logistic' - бинарная логистическая регрессия\n",
    "\n",
    "        step_alpha - float, параметр выбора шага из текста задания\n",
    "\n",
    "        step_beta- float, параметр выбора шага из текста задания\n",
    "\n",
    "        tolerance - точность, по достижении которой, необходимо прекратить оптимизацию.\n",
    "        Необходимо использовать критерий выхода по модулю разности соседних значений функции:\n",
    "        если |f(x_{k+1}) - f(x_{k})| < tolerance: то выход \n",
    "\n",
    "        max_iter - максимальное число итераций     \n",
    "\n",
    "        **kwargs - аргументы, необходимые для инициализации   \n",
    "        \"\"\"\n",
    "        if loss_function == 'binary_logistic':\n",
    "            self.oracle = oracles.BinaryLogistic(l2_coef=kwargs['l2_coef'])\n",
    "        self.alpha = step_alpha\n",
    "        self.beta = step_beta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, w_0=None, trace=False):\n",
    "        \"\"\"\n",
    "        Обучение метода по выборке X с ответами y\n",
    "\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "\n",
    "        y - одномерный numpy array\n",
    "\n",
    "        w_0 - начальное приближение в методе\n",
    "\n",
    "        trace - переменная типа bool\n",
    "\n",
    "        Если trace = True, то метод должен вернуть словарь history, содержащий информацию \n",
    "        о поведении метода. Длина словаря history = количество итераций + 1 (начальное приближение)\n",
    "\n",
    "        history['time']: list of floats, содержит интервалы времени между двумя итерациями метода\n",
    "        history['func']: list of floats, содержит значения функции на каждой итерации\n",
    "        (0 для самой первой точки)\n",
    "        \"\"\"\n",
    "        self.w = w_0.copy()\n",
    "        w_k = w_0.copy()\n",
    "        history = {'time': [0], 'func': [self.oracle.func(X, y, w_0)]}\n",
    "        start = time.time()\n",
    "        for k in range(1, self.max_iter):\n",
    "            w_k1 = w_k - self.alpha / (k ** self.beta) * self.oracle.grad(X, y,\n",
    "                                                                          w_k)\n",
    "\n",
    "            if trace:\n",
    "                history['func'].append(self.oracle.func(X, y, w_k1))\n",
    "                history['time'].append(time.time() - start)\n",
    "                start = time.time()\n",
    "            if np.absolute(self.oracle.func(X, y, w_k1) -\n",
    "                           self.oracle.func(X, y, w_k)) < self.tolerance:\n",
    "                break\n",
    "            w_k = w_k1.copy()\n",
    "        self.w = w_k.copy()\n",
    "        if trace:\n",
    "            return history\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Получение меток ответов на выборке X\n",
    "\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "\n",
    "        return: одномерный numpy array с предсказаниями\n",
    "        \"\"\"\n",
    "        if sc.sparse.issparse(X):\n",
    "            return np.sign(np.dot(self.w, X.todense().T))\n",
    "        else:\n",
    "            return np.sign(np.dot(self.w, X.T))\n",
    "        pass\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Получение вероятностей принадлежности X к классу k\n",
    "\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "\n",
    "        return: двумерной numpy array, [i, k] значение соответветствует вероятности\n",
    "        принадлежности i-го объекта к классу k \n",
    "        \"\"\"\n",
    "        if sc.sparse.issparse(X):\n",
    "            return np.vstack((np.log(1 + np.exp(np.dot(self.w,\n",
    "                                                       X.todense().T))),\n",
    "                              np.log(1 + np.exp(-np.dot(self.w,\n",
    "                                                        X.todense().T)))))\n",
    "        else:\n",
    "            return np.vstack((np.log(1 + np.exp(np.dot(self.w, X.T))),\n",
    "                              np.log(1 + np.exp(-np.dot(self.w, X.T)))))\n",
    "        pass\n",
    "\n",
    "    def get_objective(self, X, y):\n",
    "        \"\"\"\n",
    "        Получение значения целевой функции на выборке X с ответами y\n",
    "\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "\n",
    "        return: float\n",
    "        \"\"\"\n",
    "        return self.oracle.func(X, y, self.w)\n",
    "        pass\n",
    "\n",
    "    def get_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        Получение значения градиента функции на выборке X с ответами y\n",
    "\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "        y - одномерный numpy array\n",
    "\n",
    "        return: numpy array, размерность зависит от задачи\n",
    "        \"\"\"\n",
    "        return self.oracle.grad(X, y, self.w)\n",
    "        pass\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Получение значения весов функционала\n",
    "        \"\"\" \n",
    "        return self.w\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGDClassifier(GDClassifier):\n",
    "    \"\"\"\n",
    "    Реализация метода стохастического градиентного спуска для произвольного\n",
    "    оракула, соответствующего спецификации оракулов из модуля oracles.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loss_function, batch_size, step_alpha=1, step_beta=0, \n",
    "                 tolerance=1e-5, max_iter=1000, random_seed=153, **kwargs):\n",
    "        \"\"\"\n",
    "        loss_function - строка, отвечающая за функцию потерь классификатора. \n",
    "        Может принимать значения:\n",
    "        - 'binary_logistic' - бинарная логистическая регрессия\n",
    "\n",
    "        batch_size - размер подвыборки, по которой считается градиент\n",
    "\n",
    "        step_alpha - float, параметр выбора шага из текста задания\n",
    "\n",
    "        step_beta- float, параметр выбора шага из текста задания\n",
    "\n",
    "        tolerance - точность, по достижении которой, необходимо прекратить оптимизацию\n",
    "        Необходимо использовать критерий выхода по модулю разности соседних значений функции:\n",
    "        если |f(x_{k+1}) - f(x_{k})| < tolerance: то выход \n",
    "\n",
    "\n",
    "        max_iter - максимальное число итераций (эпох)\n",
    "\n",
    "        random_seed - в начале метода fit необходимо вызвать np.random.seed(random_seed).\n",
    "        Этот параметр нужен для воспроизводимости результатов на разных машинах.\n",
    "\n",
    "        **kwargs - аргументы, необходимые для инициализации\n",
    "        \"\"\"\n",
    "        super().__init__(loss_function=loss_function, step_alpha=step_alpha,\n",
    "                         step_beta=step_beta, tolerance=tolerance,\n",
    "                         max_iter=max_iter, l2_coef=kwargs['l2_coef'])\n",
    "        self.batch_size = batch_size\n",
    "        self.random_seed = random_seed\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, w_0=None, trace=False, log_freq=1):\n",
    "        \"\"\"\n",
    "        Обучение метода по выборке X с ответами y\n",
    "\n",
    "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
    "\n",
    "        y - одномерный numpy array\n",
    "\n",
    "        w_0 - начальное приближение в методе\n",
    "\n",
    "        Если trace = True, то метод должен вернуть словарь history, содержащий информацию \n",
    "        о поведении метода. Если обновлять history после каждой итерации, метод перестанет \n",
    "        превосходить в скорости метод GD. Поэтому, необходимо обновлять историю метода лишь\n",
    "        после некоторого числа обработанных объектов в зависимости от приближённого номера эпохи.\n",
    "        Приближённый номер эпохи:\n",
    "            {количество объектов, обработанных методом SGD} / {количество объектов в выборке}\n",
    "\n",
    "        log_freq - float от 0 до 1, параметр, отвечающий за частоту обновления. \n",
    "        Обновление должно проиходить каждый раз, когда разница между двумя значениями приближённого номера эпохи\n",
    "        будет превосходить log_freq.\n",
    "\n",
    "        history['epoch_num']: list of floats, в каждом элементе списка будет записан приближённый номер эпохи:\n",
    "        history['time']: list of floats, содержит интервалы времени между двумя соседними замерами\n",
    "        history['func']: list of floats, содержит значения функции после текущего приближённого номера эпохи\n",
    "        history['weights_diff']: list of floats, содержит квадрат нормы разности векторов весов с соседних замеров\n",
    "        (0 для самой первой точки)\n",
    "        \"\"\"\n",
    "        if sc.sparse.issparse(X):\n",
    "            X = np.array(X.todense())\n",
    "\n",
    "        random.seed(self.random_seed)\n",
    "\n",
    "        ind = list(range(X.shape[0]))\n",
    "        random.shuffle(ind)\n",
    "        batches = [ind[i:i + self.batch_size] for i in range(0, len(ind),\n",
    "                   self.batch_size)]\n",
    "\n",
    "        self.w = w_0\n",
    "        w_k = w_0\n",
    "        history = {'epoch_num': [0], 'time': [0],\n",
    "                   'func': [self.oracle.func(X, y, w_0)], 'weights_diff': [0]}\n",
    "        old_epoch = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(len(batches)):\n",
    "            batch = batches[i]\n",
    "            X_b = np.array([X[j] for j in batch])\n",
    "            y_b = np.array([y[j] for j in batch])\n",
    "            epoch_num = self.batch_size * i / X.shape[0]\n",
    "            for k in range(self.max_iter):\n",
    "                w_k1 = w_k - self.alpha / (k ** self.beta) *\\\n",
    "                    self.oracle.grad(X_b, y_b, w_k)\n",
    "\n",
    "                if round(epoch_num - old_epoch, 8) > log_freq:\n",
    "                    old_epoch = epoch_num\n",
    "                    if trace:\n",
    "                        history['func'].append(self.oracle.func(X_b, y_b,\n",
    "                                                                w_k1))\n",
    "                        history['epoch_num'].append(epoch_num)\n",
    "                        history['weights_diff'].append(np.linalg.norm(\n",
    "                                                       w_k1 - w_k) ** 2)\n",
    "                        history['time'].append(time.time() - start)\n",
    "                        start = time.time()\n",
    "                if np.absolute(self.oracle.func(X_b, y_b, w_k1) -\n",
    "                               self.oracle.func(X_b, y_b, w_k)) <\\\n",
    "                   self.tolerance:\n",
    "                    break\n",
    "                w_k = w_k1.copy()\n",
    "            self.w = w_k\n",
    "        if trace:\n",
    "            return history\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599454 0.6926868443805101 0.6925235796993467 0.6924122120106758 0.6923275050752085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00760802, -0.00994394, -0.02209522,  0.01089251, -0.01579711,\n",
       "       -0.00945244,  0.00085363, -0.00370373, -0.02396217, -0.02186186])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "clf = GDClassifier(loss_function='binary_logistic', step_alpha=1,\n",
    "    step_beta=0, tolerance=1e-4, max_iter=5, l2_coef=0.1)\n",
    "l, d = 1000, 10\n",
    "X = np.random.random((l, d))\n",
    "y = np.random.randint(0, 2, l) * 2 - 1\n",
    "w = np.random.random(d)\n",
    "history = clf.fit(X, y, w_0=np.zeros(d), trace=True)\n",
    "print(' '.join([str(x) for x in history['func']]))\n",
    "clf.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599454 0.6931471805599454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "clf = GDClassifier(loss_function='binary_logistic', step_alpha=1,\n",
    "    step_beta=0, tolerance=1e-4, max_iter=5, l2_coef=0.1)\n",
    "l, d = 1000, 10\n",
    "X = np.random.random((l, d))\n",
    "X = sparse.csr_matrix(X)\n",
    "y = np.random.randint(0, 2, l) * 2 - 1\n",
    "w = np.random.random(d)\n",
    "history = clf.fit(X, y, w_0=np.zeros(d), trace=True)\n",
    "print(' '.join([str(x) for x in history['func']]))\n",
    "clf.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.3, 0.6, 0.9]\n",
      "[0, 0.009053945541381836, 0.010951519012451172, 0.01104116439819336]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.03978721, -0.18664254, -0.13793378, -0.02476182, -0.15549331,\n",
       "        0.01586502, -0.21661481, -0.2480776 , -0.11059034, -0.14013816])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "clf = SGDClassifier(loss_function='binary_logistic', step_alpha=1,\n",
    "    step_beta=0, tolerance=1e-4, max_iter=5, l2_coef=0.1, batch_size=50)\n",
    "l, d = 1000, 10\n",
    "X = np.random.random((l, d))\n",
    "y = np.random.randint(0, 2, l) * 2 - 1\n",
    "w = np.random.random(d)\n",
    "history = clf.fit(X, y, w_0=np.zeros(d), trace=True, log_freq=0.299999)\n",
    "# print(' '.join([str(x) for x in history['func']]))\n",
    "print(history['epoch_num'])\n",
    "print(history['time'])\n",
    "clf.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GDClassifier(loss_function='binary_logistic', step_alpha=1,\n",
    "    step_beta=0, tolerance=1e-4, max_iter=5, l2_coef=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599454 0.6926868443805101 0.6925235796993467 0.6924122120106758 0.6923275050752085\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X, y, w_0=np.zeros(3), trace=True)\n",
    "print(' '.join([str(x) for x in history['func']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.54463117e-13],\n",
       "       [2.90000000e+01]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([[-23, -5, -1]])\n",
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6924122120106758"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_objective(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17758071, 0.20719767, 0.2195477 , 0.25327266, 0.27405911,\n",
       "       0.18292316, 0.18354796, 0.27731672, 0.18651178, 0.18408566])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_gradient(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00760802, -0.00994394, -0.02209522,  0.01089251, -0.01579711,\n",
       "       -0.00945244,  0.00085363, -0.00370373, -0.02396217, -0.02186186])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss_function='binary_logistic', batch_size=5, step_alpha=1,\n",
    "    step_beta=0, tolerance=1e-4, max_iter=5, l2_coef=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def BatchGenerator(sequence, batch_size):\n",
    "    offset = 0\n",
    "    while offset < len(sequence):\n",
    "        ret_batch = sequence[offset: batch_size + offset]\n",
    "        offset += batch_size\n",
    "        yield ret_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = range(X.shape[0])\n",
    "ind = list(range(len(seq)))\n",
    "random.shuffle(ind)\n",
    "batch_size = 3\n",
    "# bg = BatchGenerator(ind, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0, 7), (2, 4, 9), (1, 3, 8)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*[iter(ind)] * batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "# print(seq[bg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [ind[i:i + batch_size] for i in range(0, len(ind), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'e', 'b']\n",
      "['d', 'h', 'c']\n",
      "['f', 'a', 'g']\n",
      "['j']\n"
     ]
    }
   ],
   "source": [
    "for batch in batches:\n",
    "    print([seq[j] for j in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1000,) (5,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-fe8589e19a9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ml2_coef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m grad = - 1 / y.size * np.sum(np.exp(-y * np.dot(w, X.T)).reshape(-1, 1) * y.reshape(-1, 1) * X /\n\u001b[0m\u001b[0;32m      3\u001b[0m     np.array(1 + np.exp(-y * np.dot(w, X.T))).reshape(-1, 1),axis=0) + l2_coef * w\n\u001b[0;32m      4\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1000,) (5,) "
     ]
    }
   ],
   "source": [
    "l2_coef = 0.1\n",
    "grad = - 1 / y.size * np.sum(np.exp(-y * np.dot(w, X.T)).reshape(-1, 1) * y.reshape(-1, 1) * X /\n",
    "    np.array(1 + np.exp(-y * np.dot(w, X.T))).reshape(-1, 1),axis=0) + l2_coef * w\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77132064, 0.02075195, 0.63364823, 0.74880388, 0.49850701,\n",
       "        0.22479665, 0.19806286, 0.76053071, 0.16911084, 0.08833981],\n",
       "       [0.68535982, 0.95339335, 0.00394827, 0.51219226, 0.81262096,\n",
       "        0.61252607, 0.72175532, 0.29187607, 0.91777412, 0.71457578],\n",
       "       [0.54254437, 0.14217005, 0.37334076, 0.67413362, 0.44183317,\n",
       "        0.43401399, 0.61776698, 0.51313824, 0.65039718, 0.60103895],\n",
       "       [0.8052232 , 0.52164715, 0.90864888, 0.31923609, 0.09045935,\n",
       "        0.30070006, 0.11398436, 0.82868133, 0.04689632, 0.62628715],\n",
       "       [0.54758616, 0.819287  , 0.19894754, 0.8568503 , 0.35165264,\n",
       "        0.75464769, 0.29596171, 0.88393648, 0.32551164, 0.1650159 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599454 0.6922962722644881 0.6921673401771434 0.6921043596671055\n",
      "Wall time: 28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(17)\n",
    "clf = GDClassifier(loss_function='binary_logistic', step_alpha=1,\n",
    "    step_beta=0, tolerance=1e-4, max_iter=5, l2_coef=0.1)\n",
    "l, d = 1000, 10\n",
    "X = np.random.random((l, d))\n",
    "y = np.random.randint(0, 2, l) * 2 - 1\n",
    "w = np.random.random(d)\n",
    "history = clf.fit(X, y, w_0=np.zeros(d), trace=True)\n",
    "print(' '.join([str(x) for x in history['func']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
